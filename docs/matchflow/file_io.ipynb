{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9a4616",
   "metadata": {},
   "source": [
    "# Working with Files: Input & Output\n",
    "\n",
    "Flow makes it easy to **load, stream, and save structured JSON data** from a variety of sources. Whether you‚Äôre pulling from disk, an API, or a folder of `.jsonl` files ‚Äî Flow provides a consistent, lazy interface for building pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## üì• Loading Data into Flow\n",
    "\n",
    "Use `Flow.from_*` methods to create a new Flow from Python objects or files.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† From Python Data: `.from_records(...)`\n",
    "\n",
    "```python\n",
    "from penaltyblog.matchflow import Flow\n",
    "\n",
    "data = [{\"id\": 1, \"value\": \"A\"}, {\"id\": 2, \"value\": \"B\"}]\n",
    "flow = Flow.from_records(data)\n",
    "```\n",
    "\n",
    "Also works with single dicts or generators:\n",
    "\n",
    "```python\n",
    "flow = Flow.from_records({\"id\": 3, \"value\": \"C\"})\n",
    "\n",
    "def gen():\n",
    "    for i in range(3):\n",
    "        yield {\"id\": i}\n",
    "\n",
    "flow = Flow.from_records(gen())\n",
    "```\n",
    "> ‚ö†Ô∏è If you mutate records (e.g. with `.assign()`), Flow modifies them in place. Use `.copy()` or `deepcopy()` to protect your originals.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÑ From JSON Lines (JSONL) File: `.from_jsonl(...)`\n",
    "\n",
    "```python\n",
    "flow = Flow.from_jsonl(\"data/events.jsonl\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ From Folder of JSON Files: `.from_folder(...)`\n",
    "\n",
    "```python\n",
    "flow = Flow.from_folder(\"data/events/\")\n",
    "```\n",
    "\n",
    "Reads all `.json` and `.jsonl` files in a directory.\n",
    "\n",
    "Each `.json` file must contain either:\n",
    "\n",
    "- A single dict\n",
    "- A list of dicts\n",
    "- Files are streamed one at a time - efficient for bulk ingestion.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® From Glob Pattern: `.from_glob(...)`\n",
    "\n",
    "```python\n",
    "flow = Flow.from_glob(\"data/**/*.json\")\n",
    "```\n",
    "\n",
    "Searches recursively using `glob.glob`. Same behavior as `.from_folder`, but more flexible for matching paths and subfolders.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ From JSON File (Single Object or Array): `.from_json(...)`\n",
    "\n",
    "```python\n",
    "flow = Flow.from_json(\"data/game.json\")\n",
    "```\n",
    "\n",
    "- Accepts a single object (as one record), or\n",
    "- A list of objects (as multiple records)\n",
    "\n",
    "> This reads the entire file into memory. Use `.from_jsonl()` for streaming large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## Working with Cloud Storage (S3, GCS, Azure)\n",
    "\n",
    "All file-based creation methods (`from_json`, `from_jsonl`, `from_folder`, `from_glob`) can read directly from cloud storage by providing the appropriate URI and storage_options.\n",
    "\n",
    "To do this, you'll need to install the necessary dependencies for your cloud provider:\n",
    "\n",
    "- **Amazon S3**: pip install penaltyblog[aws]\n",
    "- **Google Cloud Storage**: pip install penaltyblog[gcp]\n",
    "- **Azure Data Lake / Blob Storage**: pip install penaltyblog[azure]\n",
    "\n",
    "The `storage_options` parameter is an optional dictionary containing your credentials if you are not storing them as environment variables.\n",
    "\n",
    "```python\n",
    "import penaltyblog as pb\n",
    "\n",
    "s3_options = {\n",
    "    \"key\": \"YOUR_AWS_ACCESS_KEY_ID\",\n",
    "    \"secret\": \"YOUR_AWS_SECRET_ACCESS_KEY\",\n",
    "}\n",
    "flow = pb.Flow.from_json(\"s3://my-bucket/data.json\", storage_options=s3_options)\n",
    "\n",
    "gcs_options = {\"token\": \"path/to/your/gcs_credentials.json\"}\n",
    "flow = pb.Flow.from_jsonl(\"gs://my-gcs-bucket/data.jsonl\", storage_options=gcs_options)\n",
    "\n",
    "azure_options = {\n",
    "    \"account_name\": \"YOUR_STORAGE_ACCOUNT_NAME\",\n",
    "    \"account_key\": \"YOUR_STORAGE_ACCOUNT_KEY\",\n",
    "}\n",
    "flow = pb.Flow.from_folder(\"abfs://container/data/\", storage_options=azure_options)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üíæ Saving Data from a Flow\n",
    "\n",
    "Once your pipeline is complete, use `.to_*()` methods to export the result.\n",
    "\n",
    "### `.to_jsonl(path)`\n",
    "\n",
    "Write one record per line:\n",
    "\n",
    "```python\n",
    "flow.to_jsonl(\"output/events.jsonl\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `.to_json(path)`\n",
    "\n",
    "Write all records as a JSON array:\n",
    "\n",
    "```python\n",
    "flow.to_json(\"summary.json\", indent=4)\n",
    "```\n",
    "\n",
    "> This collects the entire stream before writing.\n",
    "\n",
    "---\n",
    "\n",
    "### `.to_json_files(folder, by=\"id\")`\n",
    "\n",
    "Write each record to its own .json file:\n",
    "\n",
    "```python\n",
    "flow.to_json_files(\"out/\", by=\"event_id\")\n",
    "```\n",
    "\n",
    "- \"out/123.json\"\n",
    "- \"out/456.json\"\n",
    "\n",
    "Field must be a string or something serializable to filename.\n",
    "\n",
    "---\n",
    "\n",
    "### `.to_pandas()`\n",
    "\n",
    "Convert the flow to a Pandas DataFrame:\n",
    "\n",
    "```python\n",
    "df = flow.select(\"player_name\", \"shot_xg\").to_pandas()\n",
    "```\n",
    "\n",
    "> Best used after filtering/flattening to avoid deeply nested fields.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Source Format    | Method                | Streaming? | Notes                        |\n",
    "| ---------------- | --------------------- | ---------- | ---------------------------- |\n",
    "| Python objects   | `.from_records()`     | ‚úÖ          | Lists, dicts, or generators  |\n",
    "| JSONL file       | `.from_jsonl()`       | ‚úÖ          | Efficient for large datasets |\n",
    "| Single JSON file | `.from_json()`        | ‚ùå          | Loads entire file at once    |\n",
    "| Folder of files  | `.from_folder()`      | ‚úÖ          | Streams one file at a time   |\n",
    "| Glob pattern     | `.from_glob()`        | ‚úÖ          | Recursively matches files    |\n",
    "| StatsBomb GitHub | `.statsbomb.from_...` | ‚úÖ          | Downloads open match data    |\n",
    "\n",
    "## üì§ Summary of Output Options\n",
    "\n",
    "| Output Method      | Format          | Streaming? | Notes                         |\n",
    "| ------------------ | --------------- | ---------- | ----------------------------- |\n",
    "| `.to_jsonl()`      | JSONL           | ‚úÖ          | One line per record           |\n",
    "| `.to_json()`       | JSON array      | ‚ùå          | Collects before writing       |\n",
    "| `.to_json_files()` | Folder of files | ‚úÖ          | One file per record           |\n",
    "| `.to_pandas()`     | DataFrame       | ‚ùå          | Collects all data into memory |\n",
    "\n",
    "## üß† What‚Äôs Next?\n",
    "\n",
    "Now that you can load and save data, let‚Äôs look at inspecting, debugging, and explaining your flows using `.head()`, `.keys()`, `.explain()` and more."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
